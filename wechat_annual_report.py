import os
import sys
import pandas as pd
import jieba
import jieba.analyse
import re
from collections import Counter
from snownlp import SnowNLP
import streamlit as st
import logging
import emoji
from reportlab.lib.pagesizes import A4
from reportlab.pdfgen import canvas
from io import BytesIO

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("annual_report.log", encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

# 1. åŠ è½½åœç”¨è¯
@st.cache_data
def load_stopwords(filepath='stopwords.txt'):
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            stopwords = set(line.strip() for line in f if line.strip())
        logging.info(f"æˆåŠŸåŠ è½½åœç”¨è¯ï¼Œå…± {len(stopwords)} ä¸ªåœç”¨è¯ã€‚")
        return stopwords
    except FileNotFoundError:
        logging.error(f"åœç”¨è¯æ–‡ä»¶ {filepath} æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨ã€‚")
        st.error(f"åœç”¨è¯æ–‡ä»¶ {filepath} æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨ã€‚")
        sys.exit(1)
    except Exception as e:
        logging.error(f"åŠ è½½åœç”¨è¯æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        st.error(f"åŠ è½½åœç”¨è¯æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        sys.exit(1)

# 2. æ•°æ®æ¸…æ´—å‡½æ•°
def clean_data(df):
    """ æ¸…æ´—æ•°æ®ï¼šè½¬æ¢æ—¥æœŸæ—¶é—´æ ¼å¼ï¼Œå»é™¤æ— æ•ˆè®°å½• """
    try:
        df['DateTime'] = pd.to_datetime(df['DateTime'], errors='coerce')
        missing_dates = df['DateTime'].isna().sum()
        if missing_dates > 0:
            logging.warning(f"{missing_dates} æ¡è®°å½•çš„æ—¥æœŸæ ¼å¼æ— æ³•è§£æï¼Œå°†è¢«å¿½ç•¥ã€‚")
            st.warning(f"{missing_dates} æ¡è®°å½•çš„æ—¥æœŸæ ¼å¼æ— æ³•è§£æï¼Œå°†è¢«å¿½ç•¥ã€‚")
        df = df.dropna(subset=['DateTime'])
        df['Message'] = df['Message'].astype(str).str.strip()
        return df
    except Exception as e:
        logging.error(f"æ•°æ®æ¸…æ´—æ—¶å‡ºé”™: {e}")
        st.error(f"æ•°æ®æ¸…æ´—æ—¶å‡ºé”™: {e}")
        return pd.DataFrame()

# 3. æ–‡æœ¬é¢„å¤„ç†å‡½æ•°
def preprocess_text(text):
    """ ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å»é™¤æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦ï¼Œä»…ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’Œç©ºæ ¼ """
    if isinstance(text, str):
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', '', text)
        return text
    return ''

# 4. åˆ†è¯ä¸åœç”¨è¯è¿‡æ»¤
def tokenize_and_filter(df, stopwords):
    """ ä½¿ç”¨jiebaåˆ†è¯å¹¶è¿‡æ»¤æ‰åœç”¨è¯ """
    try:
        df['Processed_Message'] = df['Message'].apply(preprocess_text)
        df['Tokenized_Message'] = df['Processed_Message'].apply(lambda x: jieba.lcut(x))
        df['Filtered_Message'] = df['Tokenized_Message'].apply(
            lambda tokens: [word for word in tokens if word not in stopwords and word.strip()]
        )
        logging.info("åˆ†è¯ä¸åœç”¨è¯è¿‡æ»¤å®Œæˆã€‚")
        return df
    except Exception as e:
        logging.error(f"åˆ†è¯ä¸åœç”¨è¯è¿‡æ»¤æ—¶å‡ºé”™: {e}")
        st.error(f"åˆ†è¯ä¸åœç”¨è¯è¿‡æ»¤æ—¶å‡ºé”™: {e}")
        return df

# 5. è¯é¢‘ç»Ÿè®¡å‡½æ•°
def word_frequency(df):
    """ ç»Ÿè®¡è¯é¢‘å¹¶è¿”å›Counterå¯¹è±¡ """
    try:
        all_words = [word for tokens in df['Filtered_Message'] for word in tokens if word.strip() != '']
        word_freq = Counter(all_words)
        logging.info(f"è¯é¢‘ç»Ÿè®¡å®Œæˆï¼Œå…±ç»Ÿè®¡åˆ° {len(word_freq)} ä¸ªä¸åŒè¯æ±‡ã€‚")
        return word_freq
    except Exception as e:
        logging.error(f"è¯é¢‘ç»Ÿè®¡æ—¶å‡ºé”™: {e}")
        st.error(f"è¯é¢‘ç»Ÿè®¡æ—¶å‡ºé”™: {e}")
        return Counter()

# 6. æƒ…æ„Ÿåˆ†æå‡½æ•°
def sentiment_analysis(df):
    """ ä½¿ç”¨SnowNLPå¯¹æ¶ˆæ¯è¿›è¡Œæƒ…æ„Ÿåˆ†æ """
    def get_sentiment(text):
        if isinstance(text, str) and text.strip():  # ç¡®ä¿æ–‡æœ¬éç©º
            try:
                s = SnowNLP(text)
                return s.sentiments
            except Exception as e:
                logging.error(f"æƒ…æ„Ÿåˆ†ææ—¶å‡ºé”™: {e}")
                return None
        return None

    try:
        df['Sentiment'] = df['Message'].apply(get_sentiment)
        sentiment_na = df['Sentiment'].isna().sum()
        if sentiment_na > 0:
            logging.warning(f"{sentiment_na} æ¡è®°å½•çš„æƒ…æ„Ÿå¾—åˆ†ä¸º NaNã€‚")
            st.warning(f"{sentiment_na} æ¡è®°å½•çš„æƒ…æ„Ÿå¾—åˆ†ä¸º NaNã€‚")
        logging.info("æƒ…æ„Ÿåˆ†æå®Œæˆã€‚")
        return df
    except Exception as e:
        logging.error(f"æƒ…æ„Ÿåˆ†ææ—¶å‡ºé”™: {e}")
        st.error(f"æƒ…æ„Ÿåˆ†ææ—¶å‡ºé”™: {e}")
        return df

# 7. è¡¨æƒ…ç»Ÿè®¡å‡½æ•°
def extract_emojis(df):
    """
    æå–èŠå¤©è®°å½•ä¸­çš„è¡¨æƒ…å¹¶ç»Ÿè®¡é¢‘æ¬¡
    """
    try:
        # æå– Unicode è¡¨æƒ…ç¬¦å·
        def extract_emoji(text):
            return [char for char in text if char in emoji.EMOJI_DATA]

        df['Emojis'] = df['Message'].apply(lambda x: extract_emoji(x) if isinstance(x, str) else [])
        logging.info("è¡¨æƒ…æå–å®Œæˆã€‚")
        return df
    except Exception as e:
        logging.error(f"æå–è¡¨æƒ…æ—¶å‡ºé”™: {e}")
        st.error(f"æå–è¡¨æƒ…æ—¶å‡ºé”™: {e}")
        return df

def count_emojis(df):
    """
    ç»Ÿè®¡è¡¨æƒ…ä½¿ç”¨é¢‘æ¬¡å¹¶è¿”å›æ’åå‰ 5 çš„è¡¨æƒ…
    """
    try:
        all_emojis = [emoji_char for emojis in df['Emojis'] for emoji_char in emojis]
        emoji_counter = Counter(all_emojis)
        return emoji_counter.most_common(5)  # è¿”å›æ’åå‰ 5 çš„è¡¨æƒ…
    except Exception as e:
        logging.error(f"ç»Ÿè®¡è¡¨æƒ…é¢‘æ¬¡æ—¶å‡ºé”™: {e}")
        st.error(f"ç»Ÿè®¡è¡¨æƒ…é¢‘æ¬¡æ—¶å‡ºé”™: {e}")
        return []

# 8. å…³é”®æ—¶åˆ»å›é¡¾
def key_moments(df):
    """ è¯†åˆ«å…³é”®æ—¶åˆ»ï¼Œå¦‚ç”Ÿæ—¥ã€åº†ç¥ã€èšä¼šç­‰ """
    try:
        # ç¤ºä¾‹ï¼šå‡è®¾æœ‰ç‰¹å®šå…³é”®è¯æ ‡è¯†å…³é”®æ—¶åˆ»
        keywords = ['ç”Ÿæ—¥', 'åº†ç¥', 'èšä¼š', 'æ´»åŠ¨', 'åˆä½œ']
        moments = df[df['Message'].str.contains('|'.join(keywords))]
        
        # åªå–æœ€æœ‰è¶£çš„å‰5ä¸ªå…³é”®æ—¶åˆ»
        moments = moments.head(5)
    
        if not moments.empty:
            st.write("### ğŸ‰ **å…³é”®æ—¶åˆ»å›é¡¾**")
            for _, row in moments.iterrows():
                st.markdown(f"- **{row['DateTime'].strftime('%Y-%m-%d')}**: {row['Speaker']} åˆ†äº«äº† **{row['Message']}**")
        else:
            st.write("### ğŸ‰ **å…³é”®æ—¶åˆ»å›é¡¾**")
            st.write("æœªè¯†åˆ«åˆ°é‡è¦çš„å…³é”®æ—¶åˆ»ã€‚")
    except Exception as e:
        logging.error(f"è¯†åˆ«å…³é”®æ—¶åˆ»æ—¶å‡ºé”™: {e}")
        st.error(f"è¯†åˆ«å…³é”®æ—¶åˆ»æ—¶å‡ºé”™: {e}")

# 9. è¶£å‘³ç»Ÿè®¡
def fun_stats(df, word_freq):
    """ å±•ç¤ºè¶£å‘³ç»Ÿè®¡ï¼Œå¦‚æœ€å¸¸ç”¨è¯ã€æœ€é•¿æ¶ˆæ¯ã€æœ€æ´»è·ƒæ—¥æœŸç­‰ """
    try:
        # æœ€å¸¸ç”¨è¯
        most_common = word_freq.most_common(5)

        # æœ€é•¿æ¶ˆæ¯
        df['Message_Length'] = df['Message'].apply(len)
        if not df.empty:
            longest_message = df.loc[df['Message_Length'].idxmax()]
            # æœ€æ´»è·ƒæ—¥æœŸ
            daily_messages = df.groupby(df['DateTime'].dt.date).size()
            most_active_date = daily_messages.idxmax()
            most_active_count = daily_messages.max()

            st.write("### ğŸ•¹ï¸ **è¶£å‘³ç»Ÿè®¡**")
            st.markdown(f"- **æœ€å¸¸ç”¨è¯æ±‡**: {', '.join([word for word, _ in most_common])}")
            st.markdown(f"- **æœ€é•¿æ¶ˆæ¯**: \"{longest_message['Message']}\" ç”± **{longest_message['Speaker']}** äº {longest_message['DateTime'].strftime('%Y-%m-%d')} å‘é€")
            st.markdown(f"- **æœ€æ´»è·ƒæ—¥æœŸ**: {most_active_date}ï¼Œå‘é€äº† **{most_active_count} æ¡æ¶ˆæ¯**")
        else:
            st.write("### ğŸ•¹ï¸ **è¶£å‘³ç»Ÿè®¡**")
            st.write("æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œè¶£å‘³ç»Ÿè®¡ã€‚")
    except Exception as e:
        logging.error(f"å±•ç¤ºè¶£å‘³ç»Ÿè®¡æ—¶å‡ºé”™: {e}")
        st.error(f"å±•ç¤ºè¶£å‘³ç»Ÿè®¡æ—¶å‡ºé”™: {e}")

# 10. ç”ŸæˆPDFæŠ¥å‘Š
def generate_pdf(chat_name, year, report_content):
    """ ä½¿ç”¨reportlabç”ŸæˆPDFæŠ¥å‘Š """
    try:
        buffer = BytesIO()
        c = canvas.Canvas(buffer, pagesize=A4)
        width, height = A4
        margin = 50
        y_position = height - margin

        # è®¾ç½®æ ‡é¢˜
        c.setFont("Helvetica-Bold", 20)
        c.drawCentredString(width / 2, y_position, f"ç¾¤èŠã€{chat_name}ã€‘{year}å¹´åº¦æŠ¥å‘Š")
        y_position -= 40

        # è®¾ç½®æ­£æ–‡
        c.setFont("Helvetica", 12)
        for line in report_content.split('\n'):
            if y_position < margin:
                c.showPage()
                y_position = height - margin
                c.setFont("Helvetica", 12)
            c.drawString(margin, y_position, line)
            y_position -= 15

        c.save()
        buffer.seek(0)
        return buffer
    except Exception as e:
        logging.error(f"ç”ŸæˆPDFæ—¶å‡ºé”™: {e}")
        st.error(f"ç”ŸæˆPDFæ—¶å‡ºé”™: {e}")
        return None

# 11. ç”Ÿæˆå¹´åº¦æŠ¥å‘Š
@st.cache_data
def load_data(uploaded_file):
    try:
        df = pd.read_csv(uploaded_file, encoding='utf-8')
        logging.info("æˆåŠŸè¯»å–ä¸Šä¼ çš„æ–‡ä»¶ã€‚")
        return df
    except FileNotFoundError:
        st.error(f"æ–‡ä»¶ {uploaded_file.name} æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿æ–‡ä»¶è·¯å¾„æ­£ç¡®ã€‚")
        logging.error(f"æ–‡ä»¶ {uploaded_file.name} æœªæ‰¾åˆ°ã€‚")
        return pd.DataFrame()
    except pd.errors.EmptyDataError:
        st.error(f"æ–‡ä»¶ {uploaded_file.name} æ˜¯ç©ºçš„ã€‚")
        logging.error(f"æ–‡ä»¶ {uploaded_file.name} æ˜¯ç©ºçš„ã€‚")
        return pd.DataFrame()
    except Exception as e:
        st.error(f"è¯»å–æ–‡ä»¶ {uploaded_file.name} æ—¶å‡ºé”™: {e}")
        logging.error(f"è¯»å–æ–‡ä»¶ {uploaded_file.name} æ—¶å‡ºé”™: {e}")
        return pd.DataFrame()

def generate_annual_report(uploaded_file, stopwords, chat_name, year):
    df = load_data(uploaded_file)
    if df.empty:
        st.error("ä¸Šä¼ çš„æ–‡ä»¶æ²¡æœ‰æœ‰æ•ˆæ•°æ®ã€‚")
        return

    # æ£€æŸ¥å¿…è¦çš„åˆ—
    required_columns = {'Speaker', 'DateTime', 'Message'}
    if not required_columns.issubset(df.columns):
        missing = required_columns - set(df.columns)
        st.error(f"CSV æ–‡ä»¶ç¼ºå°‘ä»¥ä¸‹å¿…è¦åˆ—: {', '.join(missing)}")
        logging.error(f"CSV æ–‡ä»¶ç¼ºå°‘ä»¥ä¸‹å¿…è¦åˆ—: {', '.join(missing)}")
        return

    # æ•°æ®æ¸…æ´—
    df = clean_data(df)
    if df.empty:
        st.error("æ•°æ®æ¸…æ´—åæ²¡æœ‰æœ‰æ•ˆæ•°æ®ã€‚")
        return

    # è·å–ç¾¤èŠåˆ›å»ºæ—¥æœŸï¼ˆç¬¬ä¸€æ¡æ¶ˆæ¯æ—¥æœŸï¼‰
    creation_date = df['DateTime'].min().date()
    st.write(f"**ç¾¤èŠåˆ›å»ºæ—¥æœŸ**ï¼š{creation_date}")

    # ç­›é€‰æŠ¥å‘Šå¹´åº¦çš„æ¶ˆæ¯
    start_date = pd.to_datetime(f"{year}-01-01")
    end_date = pd.to_datetime(f"{year}-12-31")
    if creation_date > start_date.date():
        start_date = pd.to_datetime(creation_date)
    df_year = df[(df['DateTime'] >= start_date) & (df['DateTime'] <= end_date)]
    if df_year.empty:
        st.error("åœ¨æŒ‡å®šçš„æŠ¥å‘Šå¹´åº¦å†…æ²¡æœ‰æœ‰æ•ˆçš„èŠå¤©è®°å½•ã€‚")
        return

    # åˆ†è¯ä¸åœç”¨è¯è¿‡æ»¤
    df_year = tokenize_and_filter(df_year, stopwords)
    if 'Filtered_Message' not in df_year.columns:
        st.error("åˆ†è¯ä¸åœç”¨è¯è¿‡æ»¤åç¼ºå°‘ 'Filtered_Message' åˆ—ã€‚")
        logging.error("åˆ†è¯ä¸åœç”¨è¯è¿‡æ»¤åç¼ºå°‘ 'Filtered_Message' åˆ—ã€‚")
        return

    # è¯é¢‘ç»Ÿè®¡
    word_freq = word_frequency(df_year)

    # æƒ…æ„Ÿåˆ†æ
    df_year = sentiment_analysis(df_year)

    # è¡¨æƒ…æå–ä¸ç»Ÿè®¡
    df_year = extract_emojis(df_year)
    top_emojis = count_emojis(df_year)

    # æ´»è·ƒå‘è¨€è€…
    top_speakers = df_year['Speaker'].value_counts()

    # ç”ŸæˆæŠ¥å‘Šå†…å®¹
    report_content = ""

    report_content += f"ğŸ‰ **ç¾¤èŠã€{chat_name}ã€‘{year}å¹´åº¦æŠ¥å‘Š** ğŸ‰\n\n"
    report_content += "---\n\n"
    report_content += f"## ğŸŒŸ **å¼•è¨€**\n\n"
    report_content += f"{year}å¹´ï¼Œå¯¹äºæˆ‘ä»¬äº²çˆ±çš„ç¾¤èŠã€{chat_name}ã€‘æ¥è¯´ï¼Œæ˜¯å……æ»¡æ¬¢ç¬‘ã€åˆ†äº«ä¸æˆé•¿çš„ä¸€å¹´ã€‚åœ¨è¿™ä¸€å¹´ä¸­ï¼Œæˆ‘ä»¬å…±åŒç»å†äº†æ— æ•°ç²¾å½©ç¬é—´ï¼Œå¢è¿›äº†å½¼æ­¤çš„äº†è§£ï¼Œå»ºç«‹äº†æ·±åšçš„å‹è°Šã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä¸€èµ·å›é¡¾è¿™ä¸€å¹´çš„ç²¾å½©æ—¶åˆ»ï¼Œçœ‹çœ‹æˆ‘ä»¬çš„ç¾¤èŠæœ‰å¤šä¹ˆæ´»è·ƒå’Œæœ‰è¶£å§ï¼\n\n"
    report_content += "---\n\n"

    # å¹´åº¦äº®ç‚¹
    total_messages = len(df_year)
    active_days = df_year['DateTime'].dt.date.nunique()
    peak_hour = df_year['DateTime'].dt.hour.value_counts().idxmax()

    report_content += f"## ğŸ“ˆ **å¹´åº¦äº®ç‚¹**\n\n"
    report_content += f"- **æ€»æ¶ˆæ¯é‡**ï¼šæœ¬å¹´åº¦æˆ‘ä»¬å…±å‘é€äº† **{total_messages} æ¡æ¶ˆæ¯**ï¼Œå¹³å‡æ¯å¤©çº¦ **{total_messages / active_days:.0f} æ¡**ã€‚æ— è®ºæ˜¯æ—¥å¸¸é—²èŠè¿˜æ˜¯é‡è¦è®¨è®ºï¼Œå¤§å®¶çš„ç§¯æå‚ä¸è®©ç¾¤èŠå……æ»¡äº†æ´»åŠ›ã€‚\n"
    report_content += f"- **æ´»è·ƒå¤©æ•°**ï¼šå…¨å¹´ä¸­ï¼Œæœ‰ **{active_days} å¤©** çš„æ´»è·ƒäº¤æµã€‚è¿™æ˜¾ç¤ºäº†æˆ‘ä»¬ç¾¤ä½“çš„é«˜ç²˜æ€§å’ŒæŒç»­äº’åŠ¨ã€‚\n"
    report_content += f"- **é«˜å³°æ—¶æ®µ**ï¼šæ™šä¸Š **{peak_hour} ç‚¹åˆ° {peak_hour +1} ç‚¹** æ˜¯æ¶ˆæ¯å‘é€çš„é«˜å³°æœŸï¼Œå¤§å®¶ä¸‹ç­å›å®¶åçš„è½»æ¾æ—¶å…‰æˆä¸ºäº†äº¤æµçš„é»„é‡‘æ—¶æ®µã€‚\n\n"
    report_content += "---\n\n"

    # æ´»è·ƒæˆå‘˜æ¦œå•
    report_content += f"## ğŸ† **æ´»è·ƒæˆå‘˜æ¦œå•**\n\n"
    report_content += f"### **Top 5 æ´»è·ƒå‘è¨€è€…**\n\n"
    for i, (speaker, count) in enumerate(top_speakers.head(5).items(), start=1):
        # è®¡ç®—å‘è¨€å­—æ•°
        speaker_messages = df_year[df_year['Speaker'] == speaker]['Message']
        total_words = speaker_messages.apply(len).sum()
        # å‡è®¾ä¸€ç¯‡æœ¬ç§‘æ¯•ä¸šè®ºæ–‡ä¸º20,000å­—
        theses = total_words / 20000
        report_content += f"{i}. **{speaker}** - **{count} æ¡æ¶ˆæ¯**, å‘è¨€å­—æ•°ï¼š**{total_words} å­—**ï¼Œç›¸å½“äº **{theses:.2f} ç¯‡æœ¬ç§‘æ¯•ä¸šè®ºæ–‡**\n"
    report_content += f"*æ„Ÿè°¢è¿™äº›å°ä¼™ä¼´ä»¬çš„çƒ­æƒ…å‚ä¸å’Œä¸æ‡ˆè´¡çŒ®ï¼Œè®©æˆ‘ä»¬çš„ç¾¤èŠå¦‚æ­¤ç²¾å½©ï¼*\n\n"
    report_content += "---\n\n"

    # å¹´åº¦çƒ­é—¨è¯é¢˜
    report_content += f"## ğŸ§© **å¹´åº¦çƒ­é—¨è¯é¢˜**\n\n"
    top_topics = word_freq.most_common(10)
    for i, (topic, freq) in enumerate(top_topics, start=1):
        report_content += f"{i}. **{topic}** - **{freq} æ¬¡**\n"
    report_content += f"*è¿™äº›è¯é¢˜ä¸ä»…ä¸°å¯Œäº†æˆ‘ä»¬çš„èŠå¤©å†…å®¹ï¼Œä¹Ÿè®©å¤§å®¶åœ¨ä¸åŒé¢†åŸŸæœ‰æ‰€æ”¶è·å’Œæˆé•¿ã€‚*\n\n"
    report_content += "---\n\n"

    # è¡¨æƒ…ä¸GIFä½¿ç”¨ç»Ÿè®¡
    report_content += f"## ğŸ˜„ **è¡¨æƒ…ä½¿ç”¨ç»Ÿè®¡**\n\n"
    if top_emojis:
        for i, (emoji_char, freq) in enumerate(top_emojis, start=1):
            report_content += f"{i}. **{emoji_char}** - **{freq} æ¬¡**\n"
    else:
        report_content += f"æœ¬å¹´åº¦æœªä½¿ç”¨ä»»ä½•è¡¨æƒ…ç¬¦å·ã€‚\n"
    report_content += f"*è¡¨æƒ…çš„æ´»è·ƒä½¿ç”¨ï¼Œæå¤§åœ°æå‡äº†æˆ‘ä»¬çš„äº’åŠ¨ä½“éªŒï¼Œè®©æ¯ä¸€æ¡æ¶ˆæ¯éƒ½å……æ»¡äº†æƒ…æ„Ÿå’Œä¹è¶£ã€‚*\n\n"
    report_content += "---\n\n"

    # æƒ…æ„Ÿåˆ†æ
    average_sentiment = df_year['Sentiment'].mean()
    positive_messages = (df_year['Sentiment'] > 0.6).sum()
    negative_messages = (df_year['Sentiment'] < 0.4).sum()

    report_content += f"## ğŸ’¬ **æƒ…æ„Ÿåˆ†æ**\n\n"
    report_content += f"- **å¹³å‡æƒ…æ„Ÿå¾—åˆ†**ï¼š**{average_sentiment:.2f}**ï¼ˆ1.0 ä¸ºæœ€ç§¯æï¼‰\n"
    report_content += f"- **ç§¯ææ¶ˆæ¯**ï¼š**{positive_messages} æ¡**\n"
    report_content += f"- **æ¶ˆææ¶ˆæ¯**ï¼š**{negative_messages} æ¡**\n"
    report_content += f"*ç§¯æçš„æƒ…æ„Ÿæ°›å›´æ˜¯æˆ‘ä»¬ç¾¤èŠæŒç»­å¥åº·å‘å±•çš„å…³é”®ï¼Œæ„Ÿè°¢å¤§å®¶çš„åŠªåŠ›ä¸åŒ…å®¹ï¼*\n\n"
    report_content += "---\n\n"

    # å…³é”®æ—¶åˆ»å›é¡¾
    key_moments(df_year)

    report_content += "---\n\n"

    # è¶£å‘³ç»Ÿè®¡
    fun_stats(df_year, word_freq)

    report_content += "---\n\n"

    # æœªæ¥å±•æœ›
    report_content += f"## ğŸ”® **æœªæ¥å±•æœ›**\n\n"
    report_content += f"- **å¢å¼ºäº’åŠ¨**ï¼šé€šè¿‡æ›´å¤šæœ‰è¶£çš„æ´»åŠ¨å’Œè¯é¢˜ï¼Œè¿›ä¸€æ­¥å¢å¼ºç¾¤èŠçš„äº’åŠ¨æ€§å’Œè¶£å‘³æ€§ã€‚\n"
    report_content += f"- **æ‹“å±•åˆä½œ**ï¼šç»§ç»­æ¨åŠ¨ç¾¤å†…åˆä½œé¡¹ç›®ï¼Œä¸ºç¤¾åŒºå’Œå¤§å®¶å¸¦æ¥æ›´å¤šå®å®åœ¨åœ¨çš„ç›Šå¤„ã€‚\n"
    report_content += f"- **æå‡ä½“éªŒ**ï¼šä¼˜åŒ–ç¾¤èŠç®¡ç†ï¼Œç¡®ä¿äº¤æµç¯å¢ƒæ›´åŠ å‹å¥½å’Œè°ã€‚\n"
    report_content += f"*è®©æˆ‘ä»¬æºæ‰‹å¹¶è¿›ï¼Œè¿æ¥æ›´åŠ ç²¾å½©å’Œå……å®çš„æ–°ä¸€å¹´ï¼*\n\n"
    report_content += "---\n\n"

    # æ„Ÿè°¢è¯
    report_content += f"## ğŸ“¢ **æ„Ÿè°¢æœ‰ä½ **\n\n"
    report_content += f"æ„Ÿè°¢æ¯ä¸€ä½ç¾¤èŠæˆå‘˜çš„å‚ä¸å’Œè´¡çŒ®ï¼Œæ˜¯ä½ ä»¬è®©ã€{chat_name}ã€‘æˆä¸ºä¸€ä¸ªæ¸©æš–ã€æœ‰è¶£ä¸”å……æ»¡æ´»åŠ›çš„å¤§å®¶åº­ã€‚æœŸå¾…åœ¨æœªæ¥çš„æ—¥å­é‡Œï¼Œæˆ‘ä»¬èƒ½ä¸€èµ·åˆ›é€ æ›´å¤šç¾å¥½çš„å›å¿†ï¼\n\n"
    report_content += "---\n\n"

    # é™„å½•
    report_content += f"# ğŸ“š **é™„å½•**\n\n"
    report_content += f"### **æˆå‘˜ç»Ÿè®¡**\n"
    total_members = df_year['Speaker'].nunique()
    active_members = df_year['Speaker'].value_counts().head(5).index.tolist()
    report_content += f"- **æ€»æˆå‘˜æ•°**ï¼š**{total_members}** äºº\n"
    report_content += f"- **æ´»è·ƒæˆå‘˜**ï¼š{', '.join(active_members)} ç­‰\n"
    report_content += f"*æˆå‘˜ç»“æ„çš„å˜åŒ–ä½“ç°äº†ç¾¤èŠçš„æ´»è·ƒåº¦å’Œå¸å¼•åŠ›ã€‚*\n\n"
    report_content += "---\n\n"

    # ç»“æŸè¯­
    report_content += f"# ğŸŒˆ **ç»“æŸè¯­**\n\n"
    report_content += f"{year}å¹´å·²ç»æˆä¸ºå†å²ï¼Œä½†æˆ‘ä»¬åœ¨ç¾¤èŠã€{chat_name}ã€‘ä¸­çš„æ•…äº‹è¿˜åœ¨ç»§ç»­ã€‚è®©æˆ‘ä»¬ä¸€èµ·æœŸå¾…ï¼Œè¿æ¥æ›´åŠ ç¾å¥½å’Œç²¾å½©çš„{year + 1}å¹´ï¼\n\n"

    # æ˜¾ç¤ºæŠ¥å‘Š
    st.markdown(report_content)

    # ç”ŸæˆPDF
    if st.button("å¯¼å‡ºä¸ºPDF"):
        pdf_buffer = generate_pdf(chat_name, year, report_content)
        if pdf_buffer:
            st.download_button(
                label="ä¸‹è½½å¹´åº¦æŠ¥å‘Š PDF",
                data=pdf_buffer,
                file_name=f"{chat_name}_{year}_Annual_Report.pdf",
                mime='application/pdf'
            )

# 10. ç”Ÿæˆå¹´åº¦æŠ¥å‘Š
def generate_report(chat_name, year, df, word_freq, top_speakers, top_emojis):
    try:
        # å†…å®¹å·²ç»åœ¨generate_annual_reportå‡½æ•°ä¸­ç”Ÿæˆ
        pass  # ç”±äºgenerate_annual_reportå·²ç»å¤„ç†æŠ¥å‘Šå†…å®¹å’ŒPDFç”Ÿæˆï¼Œè¿™é‡Œä¸éœ€è¦é¢å¤–æ“ä½œ
    except Exception as e:
        logging.error(f"ç”Ÿæˆå¹´åº¦æŠ¥å‘Šæ—¶å‡ºé”™: {e}")
        st.error(f"ç”Ÿæˆå¹´åº¦æŠ¥å‘Šæ—¶å‡ºé”™: {e}")

# ä¸»å‡½æ•°
def main():
    st.title("ğŸ‰ **ç¾¤èŠå¹´åº¦æŠ¥å‘Šç”Ÿæˆå™¨** ğŸ‰")
    st.write("ä¸Šä¼ ä½ çš„ç¾¤èŠCSVæ–‡ä»¶ï¼Œç”Ÿæˆä¸€ä»½ç”ŸåŠ¨æœ‰è¶£çš„å¹´åº¦æŠ¥å‘Šã€‚")

    st.sidebar.title("ä¸Šä¼ æ•°æ®æ–‡ä»¶")
    uploaded_file = st.sidebar.file_uploader("é€‰æ‹©ä¸€ä¸ªCSVæ–‡ä»¶", type=["csv"])
    chat_name = st.sidebar.text_input("è¯·è¾“å…¥ç¾¤èŠåç§°", "é˜¿æ‹‰å–„ç¾¤èŠ")
    year = st.sidebar.number_input("è¯·è¾“å…¥æŠ¥å‘Šå¹´åº¦", min_value=2000, max_value=2100, value=2023)

    if uploaded_file is not None:
        st.header(f"ğŸ“ **æ­£åœ¨ç”Ÿæˆã€{chat_name}ã€‘{year}å¹´åº¦æŠ¥å‘Š**")
        stopwords = load_stopwords('stopwords.txt')
        generate_annual_report(uploaded_file, stopwords, chat_name, year)
    else:
        st.write("è¯·åœ¨å·¦ä¾§ä¸Šä¼ ä¸€ä¸ªCSVæ–‡ä»¶ä»¥ç”Ÿæˆå¹´åº¦æŠ¥å‘Šã€‚")

if __name__ == "__main__":
    main()
